In this section, we describe one dimensional CNNs which are commonly used in natural language processing\citep{collobert2008unified, collobert2011natural, kalchbrenner2014convolutional, zhang2015character, goldberg2016primer}.

CNNs are made up of two parts; convolutional layers and fully-connected layers.
The role of convolutional layers is to produce the most salient information of the input \citep{goldberg2016primer}.
The output of the convolutional layers are then fed to the fully-connected layers, which perform the prediction.
We omit the description of fully-connected layers.

A convolutional layer applies three operations to its input.
The first operation is \emph{convolution}, which is the application of a linear function to a window of the input. 
The window is moved with some stride, and the convolution is applied to each window \citep{goldberg2016primer}.
%In convolution, a function is applied to a region of the input 
%In this section we describe the \emph{temporal convolution}.
%When a CNN takes images as an input, the convolution is applied in two dimension.
%Let the dimension of the input layer to be $m_i \times n_i$.
%In two dimensional convolution, the size of kernel, say $m_k \times n_k$ is such that $m_k < m_i$ and $n_k < n_i$, and the kernel is ``moved'' to both directions.
%
%In temporal convolution, which is often used in natural language processing, the convolution is applied in a single dimension.
%Each feature, a character in this report, is represented as a vector of length $m_i$ and $n_i$ of them constitute the input.
%In one dimensional convolution, the size of kernel is such that $m_k = m_i$ and the kernel is moved along the column.
Let $l$ be the number of the features which represent the input, and $k$ be the width of the sliding window.
Let $f(x)$ be a function which takes an index in the window $x \in [1, k]$ and returns the weight applied to the $x$th element.
Let $g(x)$ be a function which takes an index of the input $x \in [1, l]$ and returns the $x$th feature in the input.
Let $d$ be the stride of the convolution.
Then, the result of the convolution applied to $y$th window is expressed as the following function $h(y)$ \citep{zhang2015character}:
\begin{align*}
h(y) = \sum_{x=1}^{k} f(x) \cdot g(y \cdot d - x + c)
\end{align*}
where $c = k - d + 1$ is an offset constant.

The second operation is \emph{non-linearlity} which is applied to each result of the convolution.
This allows CNNs to model a non-linear function.
A popular choice of non-linearlity is \emph{rectifier}, defined as:
\begin{align*}
ReLU(x) = max(0, x)
\end{align*}

The last operation in a convolutional layer is \emph{pooling}.
Similarly to convolution, the pooling operation is also done with a sliding window, moved with a certain stride.
In pooling, the information in the window is aggregated by some function.
For instance, in max-pooling, the function returns the largest element in the window is used.
Another popular function for pooling returns the average of the elements in the window.
The purpose of pooling is to reduce the to possibility of overfitting by downsizing the input and reducing the number of parameters in the later layers \citep{krizhevsky2012imagenet}.

In the training, the loss is calculated from the output of the fully-connected network, and the error gradients are propagated back to the convolutional layers to adjust the weight applied by $f(x)$ in convolution \citep{goldberg2016primer}.
Thus the features that the convolutional layers extract from the input are most informative for the particular prediction task the network is trained for.
