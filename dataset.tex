We use a subset of the Cambridge Learner Corpus \citep{nicholls2003cambridge} (CLC).
CLC is a corpus which consists of scripts produced by learners with 86 different L1 for examinations provided by Cambridge English Language Assessment.
Each documents is annotated with basic information of the author, such as their L1 and age, making it an appropriate corpus for the task of author profiling.

We use a subset of the CLC FCE Dataset\cite{yannakoudakis2011new} as a test set, as it is publicly available.
This contains 1244 scripts, each containing two answers, produced by learners with 16 different L1 for the Cambridge ESOL First Certificate in English (FCE) examination in 2010 and 2011.
Table \ref{tab:l1-fce} shows the number of scripts for each L1.
\begin{table}[]
\centering
\caption{Distribution of L1 in the CLC FCE Dataset}
\label{tab:l1-fce}
\begin{tabular}{|c|c|}
\hline
L1         & Count \\ \hline
Dutch      & 2     \\ \hline
Swedish    & 15    \\ \hline
Thai       & 63    \\ \hline
Catalan    & 64    \\ \hline
Chinese    & 66    \\ \hline
Portuguese & 68    \\ \hline
German     & 69    \\ \hline
Greek      & 74    \\ \hline
Italian    & 76    \\ \hline
Polish     & 76    \\ \hline
Turkish    & 77    \\ \hline
Japanese   & 80    \\ \hline
Russian    & 82    \\ \hline
Korean     & 86    \\ \hline
French     & 146   \\ \hline
Spanish    & 200   \\ \hline
\end{tabular}
\end{table}
We extracted scripts produced by Japanese, Russian and Italian native speakers to create a test set of 476 examples.
(Recall that each script contains two answers.)
We chose these three languages for two reasons;
Firstly, the number of the scripts available for each language is fairly even.
Secondly, they belong to the different language family, which should make the classification more manageable.

We selected 3000 answers from the CLC, 1000 for each L1, for the training set.

The level of proficiency is another variable that influences the writing of a learner, and therefore may introduce an undesirable bias.
Each Cambridge ESOL examination expects a certain level of proficiency from the candidates.
The reference level is provided by Common European Framework of Reference for Languages (CEFR) \citep{council2001common} and FCE is aimed for learners at CEFR level B (i.e. independent user).
In the training set, for each language, there are 250 scripts by learners at CEFR level A and C (i.e. basic and proficient user, respectively), and 500 scripts by learners at CEFR level B.
We decided to have more scripts by learners at CEFR level B assimilate the training set to the test set.
It is also reasonable to assume that there are more independent users than basic or proficient users.

Table \ref{tab:tr-stat} and \ref{tab:te-stat} gives the statistic of the length of the documents in terms of character in training and test set respectively.
As can be seen, there is a much larger variance in training set than in test set.
This is due to the fact that the training set contains answers for 15 different exams, where the expected length of the answers are different from one another, whereas the test set only contains answers for FCE.
\begin{table}[]
\centering
\caption{Statistics of the Character Length of Documents in Training Set}
\label{tab:tr-stat}
\begin{tabular}{|c|c|}
\hline
Min         & 58 \\ \hline
Max      & 3710     \\ \hline
Median    & 376.5    \\ \hline
Average & 742.1   \\ \hline
Standard Deviation    & 716.4   \\ \hline
\end{tabular}
\end{table}
\begin{table}[]
\centering
\caption{Statistics of the Character Length of Documents in Test Set}
\label{tab:te-stat}
\begin{tabular}{|c|c|}
\hline
Min         & 685\\ \hline
Max      & 1822     \\ \hline
Median    & 1081   \\ \hline
Average & 1099    \\ \hline
Standard Deviation    & 186.8    \\ \hline
\end{tabular}
\end{table}
